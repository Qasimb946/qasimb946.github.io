# Physics Informed Neural Networks
This blog gives the idea of how Physics Informed Neural Networks work and the motivation behind it. 

## PINNs in Simple Words
"Using Neural networks architecture and customizing the Loss function by embedding physics rules as a regularized term to train a model that could know how physics works"


## Motivation
The gap between Deep Learning and Physical Systems becomes more apparent when we have limited data, and we need to make predictions outside the range of the available samples (extrapolation). This is where Physics-Informed Neural Networks (PINNs) shine and offer a compelling solution. PINNs address this issue by incorporating known physical laws or constraints directly into the learning process. By embedding physics within the neural network architecture, PINNs can exploit prior knowledge about the physical system, even when only a limited number of samples are available.

<p align="center">
  <img src="/css/pinn_feature.png" alt="Image1" width="500" height="300">
</p>

## Problem with Traditional Physics Models
Traditional physics-based methods typically involve solving partial differential equations (PDEs) or ordinary differential equations (ODEs) analytically or numerically to obtain solutions for a given physical system. While these methods are powerful and have been extensively used in scientific and engineering applications, they face several challenges when it comes to solving certain problems that PINNs can address more effectively:

### 1- Limited Analytical Solutions
Many physical systems have complex geometries or boundary conditions, making it challenging to obtain closed-form analytical solutions for the governing equations. This limitation restricts the use of traditional physics-based methods to only a few simplified scenarios.

### 2- Numerical Complexity
Numerical methods, such as finite difference, finite element, or spectral methods, can handle more complex geometries and boundary conditions. However, they often require substantial computational resources, and their accuracy depends on the grid resolution and time steps used. 

### 3- Uncertainty in Parameters
Real-world physical systems may involve uncertain parameters or missing data. Traditional physics-based methods struggle to handle uncertainty and may require additional assumptions or simplifications, potentially leading to inaccurate results.

### 4- Inability to Learn
Traditional Physics models are unable to work beyond the observed data which makes extrapolation impossible. This hampers their performance in situations with limited data.

### 5- Computationally Expensive
Non-Linear and High-Dimensional Data make numerical and analytical solutions computationally costly for a system to handle which is not a good practice. 

## Problem with Simple DNN Model
A simple deep neural network (DNN) without any physics incorporation may not be the best choice for solving Physics-Informed Neural Network (PINN) problems. Here are some reasons why a simple DNN may not perform as well as a PINN in these scenarios:

### 1- No Underlying Physics
A simple Deep Neural Network without underlying physics is just a like a car without tires. Imagine if you throw a ball and measure the values of velocity and coordinates before the first bounce completely missing what happens after the first bounce ignoring friction and other natural forces. A model trained on such data can never extrapolate the actual movement of the ball as there is no understanding of physics laws to an object's movement when friction and other forces act on it. 

### 2- Huge Data Sets
Simple DNNs typically require a large amount of data to generalize well. In many physics problems, obtaining a vast amount of data might be impractical or expensive. PINNs can leverage both limited data and governing physics to improve generalization and handle situations with sparse or noisy data effectively.

### 3- Interpretability 
Simple DNNs may lack interpretability, which is crucial in scientific applications. Understanding the underlying physical laws and the model's behavior is essential for gaining insights into the system being studied. PINNs inherently preserve physical interpretability by combining data-driven learning with explicit physics constraints.


## PINNs: Mixture of Deep Neural Network and Physics

<p align="center">
  <img src="/css/DNN_Phyiscs.webp" alt="Image" width="400" height="250">
  <center><figcaption style="font-style: italic;">(Henderson, 2022)</figcaption></center>
</p>

Combining the Power of Neural Networks with Physics to accurately predict the behavior of a physical system. PINNs do not require explicit analytical solutions to the governing partial differential equations (PDEs) or ordinary differential equations (ODEs) describing the physical system. Instead, they learn directly from the available data, which makes them well-suited for scenarios where obtaining analytical solutions is challenging or even infeasible due to complex geometries, boundary conditions, or nonlinear behaviors.




## Architecture of PINNs

<p align="center">
  <img src="/css/Arch.png" alt="Image" width="1000" height="500">
  <center><figcaption style="font-style: italic;">(Salvatore et al, 2022)</figcaption></center>
</p>

The input to the neural network is typically the spatial or temporal coordinates of the physical domain or any other relevant features. For time-dependent problems, the input can also include time. The output of the neural network is the approximation of the physical quantity of interest, such as the solution to a PDE at a specific location and time. The neural network consists of multiple layers of neurons (nodes) that are organized into input, hidden, and output layers. Each neuron in the network is associated with a weight and a bias, which are learned during the training process. During forward propagation, the input data flows through the layers of the neural network. Each neuron's output is computed by taking a weighted sum of the inputs from the previous layer, adding a bias term, and passing it through the activation function. This process continues until the final output **U(Z)** is obtained. The neural network is trained to approximate the true solutions of the underlying equations by minimizing a loss function. In PINNs, the loss function combines the data loss (how well the model fits the observed data) and the physics loss (how well the model satisfies the PDEs/ODEs constraints). The weights and biases of the neurons are updated during training using optimization algorithms such as stochastic gradient descent (SGD) or Adam.

### Loss Function of PINNs
The loss function of Physics-Informed Neural Networks (PINNs) is a crucial component that guides the training process. It is designed to incorporate the labeled data, boundary conditions, and physics-based residual terms. Let's break down the components of the PINNs loss function:

**->** Labeled Data in the above-given architecture is a Mean Square Error(MSE) between the predicted value **U(Z)** and the ground truth **U** averaged by the number of samples N. This term ensures that the neural network accurately fits the data observed during training. For example, in a regression problem with the labeled data loss term could be the mean squared error (MSE) between the predicted output ̂U(Z) and the actual output U:


<p align="center">
  <img src="/css/L_Data.png" alt="Image" width="500" height="50">
  <center><figcaption style="font-style: italic;">Loss Labeled Data</figcaption></center>
</p>

Here:
<ul>
    <!-- List Items -->
    <li>N: Total Number of Data Samples in Training Dataset</li>
    <li>θ: Model Parameter</li>
    <li>U(Zi): Predicted Value </li>
    <li>Ui: Actual Value </li>
    <!-- Add more list items as needed -->
  </ul>

In the context of Physics-Informed Neural Networks (PINNs), Labeled Data Loss is just one part of the overall loss function used during training. PINNs aim to solve partial differential equations (PDEs) or ordinary differential equations (ODEs) by combining labeled data, boundary conditions, and the physics-based residual.

**->** Boundary conditions are essential in physics and engineering problems, especially when dealing with partial differential equations (PDEs). They specify the behavior of the solution at the boundaries of the physical domain and are used to complete the mathematical model of the problem. By providing boundary conditions, we constrain the solution to the PDE within the domain of interest, making the problem well-posed. In the context of PINNs, the neural network is trained to approximate the solution to a PDE. To ensure that the neural network satisfies the specified boundary conditions during training, the Boundary Loss term is introduced in the loss function. The idea is to evaluate the difference between the predicted outputs of the neural network at the boundary points and the specified boundary conditions. The Boundary Loss penalizes any discrepancy between the predicted values and the values dictated by the boundary conditions.

<p align="center">
  <img src="/css/B_Data.png" alt="Image" width="500" height="50">
  <center><figcaption style="font-style: italic;">Loss Labeled Data</figcaption></center>
</p>

Here:
<ul>
    <!-- List Items -->
    <li>N: Total Number of Data Samples in Training Dataset</li>
    <li>θ: Model Parameter</li>
    <li>U(Zi): Predicted Value </li>
    <li>B: Boundary Conditions </li>
    <!-- Add more list items as needed -->
  </ul>

By incorporating the Boundary Loss in the overall loss function, PINNs can efficiently learn to satisfy the boundary conditions along with fitting the observed data and enforcing the physics-based residual. This ability to handle boundary conditions makes PINNs particularly well-suited for solving physical problems with complex geometries and boundary constraints, where explicit analytical solutions are often challenging to obtain.

**->** In physics-based problems, differential equations describe the relationships between various physical quantities and their rates of change. These equations represent the fundamental laws governing the behavior of the system. For example, in fluid dynamics, the Navier-Stokes equations describe the motion of fluids, and in heat conduction, the heat equation describes the flow of heat in a material. The residual is a crucial concept when dealing with differential equations. For a given PDE or ODE, the residual is the difference between the left-hand side (LHS) and the right-hand side (RHS) of the equation. In other words, it measures the "remaining" or "unbalanced" part of the equation.


<p align="center">
  <img src="/css/R_Data.png" alt="Image" width="500" height="50">
  <center><figcaption style="font-style: italic;">Loss Labeled Data</figcaption></center>
</p>

Here:
<ul>
    <!-- List Items -->
    <li>N: Total Number of Data Samples in Training Dataset</li>
    <li>θ: Model Parameter</li>
    <li>R(Ui): represents the residual calculated based on the predicted values of u(x, t) from the neural network </li>
    <!-- Add more list items as needed -->
  </ul>

The Residual Loss is crucial because it ensures that the neural network's predictions adhere to the physics equations throughout the entire domain. By incorporating this physics-based constraint into the loss function, PINNs can accurately approximate the solutions to complex physical systems, even in regions where data might be sparse or unavailable.
