# Physics Informed Neural Networks
This blog gives the idea of how Physics Informed Neural Networks work and the motivation behind it. 

## PINNs in Simple Words
"Using Neural networks architecture and customizing the Loss function by embedding physics rules as a regularized term to train a model that could know how physics works"


## Motivation
The gap between Deep Learning and Physical Systems becomes more apparent when we have limited data, and we need to make predictions outside the range of the available samples (extrapolation). This is where Physics-Informed Neural Networks (PINNs) shine and offer a compelling solution. PINNs address this issue by incorporating known physical laws or constraints directly into the learning process. By embedding physics within the neural network architecture, PINNs can exploit prior knowledge about the physical system, even when only a limited number of samples are available.

<p align="center">
  <img src="/css/pinn_feature.png" alt="Image1" width="500" height="300">
</p>

## Problem with Traditional Physics Models
Traditional physics-based methods typically involve solving partial differential equations (PDEs) or ordinary differential equations (ODEs) analytically or numerically to obtain solutions for a given physical system. While these methods are powerful and have been extensively used in scientific and engineering applications, they face several challenges when it comes to solving certain problems that PINNs can address more effectively:

### 1- Limited Analytical Solutions
Many physical systems have complex geometries or boundary conditions, making it challenging to obtain closed-form analytical solutions for the governing equations. This limitation restricts the use of traditional physics-based methods to only a few simplified scenarios.

### 2- Numerical Complexity
Numerical methods, such as finite difference, finite element, or spectral methods, can handle more complex geometries and boundary conditions. However, they often require substantial computational resources, and their accuracy depends on the grid resolution and time steps used. 

### 3- Uncertainty in Parameters
Real-world physical systems may involve uncertain parameters or missing data. Traditional physics-based methods struggle to handle uncertainty and may require additional assumptions or simplifications, potentially leading to inaccurate results.

### 4- Inability to Learn
Traditional Physics models are unable to work beyond the observed data which makes extrapolation impossible. This hampers their performance in situations with limited data.

### 5- Computationally Expensive
Non-Linear and High-Dimensional Data make numerical and analytical solutions computationally costly for a system to handle which is not a good practice. 

## Problem with Simple DNN Model
A simple deep neural network (DNN) without any physics incorporation may not be the best choice for solving Physics-Informed Neural Network (PINN) problems. Here are some reasons why a simple DNN may not perform as well as a PINN in these scenarios:

### 1- No Underlying Physics
A simple Deep Neural Network without underlying physics is just a like a car without tires. Imagine if you throw a ball and measure the values of velocity and coordinates before the first bounce completely missing what happens after the first bounce ignoring friction and other natural forces. A model trained on such data can never extrapolate the actual movement of the ball as there is no understanding of physics laws to an object's movement when friction and other forces act on it. 

### 2- Huge Data Sets
Simple DNNs typically require a large amount of data to generalize well. In many physics problems, obtaining a vast amount of data might be impractical or expensive. PINNs can leverage both limited data and governing physics to improve generalization and handle situations with sparse or noisy data effectively.

### 3- Interpretability 
Simple DNNs may lack interpretability, which is crucial in scientific applications. Understanding the underlying physical laws and the model's behavior is essential for gaining insights into the system being studied. PINNs inherently preserve physical interpretability by combining data-driven learning with explicit physics constraints.


## PINNs: Mixture of Deep Neural Network and Physics

<p align="center">
  <img src="/css/DNN_Phyiscs.webp" alt="Image" width="400" height="250">
  <center><figcaption style="font-style: italic;">(Henderson, 2022)</figcaption></center>
</p>

Combining the Power of Neural Networks with Physics to accurately predict the behavior of a physical system. PINNs do not require explicit analytical solutions to the governing partial differential equations (PDEs) or ordinary differential equations (ODEs) describing the physical system. Instead, they learn directly from the available data, which makes them well-suited for scenarios where obtaining analytical solutions is challenging or even infeasible due to complex geometries, boundary conditions, or nonlinear behaviors.




## Architecture of PINNs

<p align="center">
  <img src="/css/Arch.png" alt="Image" width="1000" height="500">
  <center><figcaption style="font-style: italic;">(Salvatore et al, 2022)</figcaption></center>
</p>

The input to the neural network is typically the spatial or temporal coordinates of the physical domain or any other relevant features. For time-dependent problems, the input can also include time. The output of the neural network is the approximation of the physical quantity of interest, such as the solution to a PDE at a specific location and time. The neural network consists of multiple layers of neurons (nodes) that are organized into input, hidden, and output layers. Each neuron in the network is associated with a weight and a bias, which are learned during the training process. During forward propagation, the input data flows through the layers of the neural network. Each neuron's output is computed by taking a weighted sum of the inputs from the previous layer, adding a bias term, and passing it through the activation function. This process continues until the final output **U** is obtained. The neural network is trained to approximate the true solutions of the underlying equations by minimizing a loss function. In PINNs, the loss function combines the data loss (how well the model fits the observed data) and the physics loss (how well the model satisfies the PDEs/ODEs constraints). The weights and biases of the neurons are updated during training using optimization algorithms such as stochastic gradient descent (SGD) or Adam.

### Loss Function of PINNs
The loss function of Physics-Informed Neural Networks (PINNs) is a crucial component that guides the training process. It is designed to incorporate the labeled data, boundary conditions, and physics-based residual terms. Let's break down the components of the PINNs loss function:

Labeled Data in the above-given architecture is a Mean Square Error(MSE) between the predicted value **U** and the ground truth **U(Z)** averaged by the number of samples N. This term ensures that the neural network accurately fits the data observed during training. For example, in a regression problem with labeled data (xᵢ, yᵢ), the labeled data loss term could be the mean squared error (MSE) between the predicted output ̂yᵢ and the actual output yᵢ:

<p>Ldata(θ) = <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mfrac linethickness="0">
      <mn>1</mn>
      <mi>n</mi>
    </mfrac>
    <munderover>
      <mo>∑</mo>
      <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
      </mrow>
      <mi>n</mi>
    </munderover>
    <mrow>
      <mo>(</mo>
      <msup>
        <mrow>
          <mi>̂y</mi>
          <mi>i</mi>
        </mrow>
        <mo>-</mo>
        <mi>y</mi>
        <mi>i</mi>
      </msup>
      <mo>)</mo>
      <msup>
        <mrow>
          <mo>²</mo>
        </mrow>
        <mo></mo>
      </msup>
    </mrow>
  </math></p>


